{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1mVeTbwGvQz"
      },
      "source": [
        ":# **NATURAL LANGUAGE PROCESSING**\\\n",
        "Natural Language Processing, or NLP for short, is broadly defined as the automatic manipulation of natural language, like speech and text, by software.\n",
        "\n",
        "The study of natural language processing has been around for more than 50 years and grew out of the field of linguistics with the rise of computers.\n",
        "\n",
        "In this , you will discover what natural language processing is and why it is so important.\n",
        "\n",
        "What natural language is and how it is different from other types of data.\n",
        "What makes working with natural language so challenging.\n",
        "Where the field of NLP came from and how it is defined by modern practitioners.\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJrcIBgpG5zh"
      },
      "source": [
        "### REGULAR EXPRESSION\n",
        "A regular expression (sometimes called a rational expression) is a sequence of characters that define a search pattern, mainly for use in pattern matching with strings, or string matching, i.e. “find and replace”-like operations.\n",
        "\n",
        "Regular expressions are specially encoded text strings used as patterns for matching sets of strings.\n",
        "\n",
        "Regular expressions are a generalized way to match patterns with sequences of characters. It is used in every programming language like C++, Java and Python.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF1w-1IBGpK0"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44xzgXGqHKkq"
      },
      "source": [
        "my_string = \"Lets write RegEx! wont that be fun? I sure think so. Can you find 4 sentence? or perhapse, all 19 words ?\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-zNZC8qHpWa"
      },
      "source": [
        "sentence_end= r\"[.?!]\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zm7x_08HyCl"
      },
      "source": [
        "print(re.split(sentence_end, my_string))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hybq848kH8XE"
      },
      "source": [
        "#split my_string on capitalized words  and print#\n",
        "capitalized_words = r\"[A-Z]\\w+\"\n",
        "print(re.findall(capitalized_words,my_string))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMjbK9x6IjXU"
      },
      "source": [
        "#split my_string on spaces and print the result#\n",
        "spaces = r\"\\s+\"\n",
        "print(re.split(spaces,my_string))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un51hLz3JgMG"
      },
      "source": [
        "#find all digit in my_string and print the result#\n",
        "digits = r\"\\d+\"\n",
        "print(re.findall(digits, my_string))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZDp3ylZJ4uC"
      },
      "source": [
        "re.match(\"b\",\"abcdef\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlqQ3ZuyPJJq"
      },
      "source": [
        "re.search(\"d\",\"abcdefg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZAzVXnGTqrC"
      },
      "source": [
        "# **TOKENIZATION**\n",
        "\n",
        "**Tokenization is the process of exchanging sensitive data for nonsensitive data called \"tokens\" that can be used in a database or internal system without bringing it into scope.\n",
        "\n",
        "Although the tokens are unrelated values, they retain certain elements of the original data—commonly length or format—so they can be used for uninterrupted business operations. The original sensitive data is then safely stored outside of the organization's internal systems.\n",
        "\n",
        "Unlike encrypted data, tokenized data is undecipherable and irreversible. This distinction is particularly important: Because there is no mathematical relationship between the token and its original number, tokens cannot be returned to their original form without the presence of additional, separately stored data. As a result, a breach of a tokenized environment will not compromise the original sensitive data.** \n",
        "\n",
        "# There are actually 4 Types of Tokenization:-\n",
        "### 1. WORD_TOKENIZE \n",
        "### 2. SENT_TOKENIZE\n",
        "### 3. REGEXP_TOKENIZE\n",
        "### 4. TWEETTOKENIZER "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MOvJA-15-Fi"
      },
      "source": [
        "# 1. WORD_TOKENIZE\n",
        "\n",
        "Word tokenization is the process of splitting a large sample of text into words. This is a requirement in natural language processing tasks where each word needs to be captured and subjected to further analysis like classifying and counting them for a particular sentiment etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjFKD2-jPP7y"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(\"Welcome to NLP\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqIM65enVIKJ"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "  \n",
        "text = \"Hello everyone. I am Yash Thavkar. We are going to study about NLP\"\n",
        "word_tokenize(text)"
      ],
      "metadata": {
        "id": "I63kRbXPqUtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAexYotL6C8R"
      },
      "source": [
        "# 2. SENT_TOKENIZE\n",
        "\n",
        "The sent_tokenize function uses an instance of PunktSentenceTokenizer from the nltk.tokenize.punkt module,\\\n",
        " which is already been trained and thus very well knows to mark the end and beginning of sentence at what characters and punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR_AUOuoWRjZ"
      },
      "source": [
        "sentence = sent_tokenize(\"scene_one\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9o6jY7UWZ6L"
      },
      "source": [
        "tokenized_sent = word_tokenize(sentence[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr-uhSWjWvLX"
      },
      "source": [
        "unique_tokens = set(word_tokenize(\"scene_one\"))\n",
        "print(unique_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chc0psDoXNXm"
      },
      "source": [
        "#only digits\n",
        "tokenize_digits_and_words = ('\\d+')\n",
        "re.findall(tokenize_digits_and_words ,\"he has 8 dogs and 11 cats\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UViTGIjmX2KC"
      },
      "source": [
        "# only alphabet\n",
        "tokenize_digits_and_words =('[a-z]+')\n",
        "re.findall(tokenize_digits_and_words, \"he has 8 dogs and 11 cats\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwkbQaAQYbPZ"
      },
      "source": [
        "#both alphabets and digits\n",
        "tokenize_digits_and_words =('\\w+')\n",
        "re.findall(tokenize_digits_and_words,\"he has 8 dogs and 11 cats\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMC4Dgn9Y-9J"
      },
      "source": [
        "my_str = (\"match lowercase spaces nums like 12, but no commas\")\n",
        "re.match('[a-z0-9]+',my_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"Hello everyone. I am Yash Thavkar. We are going to study about NLP\"\n",
        "sent_tokenize(text)"
      ],
      "metadata": {
        "id": "ePAQz5WjpX5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhABhsyX6Mtx"
      },
      "source": [
        "# 3. REGEXP_TOKENIZE\n",
        "\n",
        "Regular expressions can be used if you want complete control over how to tokenize text. As regular expressions can get complicated very quickly, I only recommend using them if the word tokenizers covered in the previous recipe are unacceptable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqNf_RgkcVVx"
      },
      "source": [
        "from nltk import regexp_tokenize\n",
        "my_str =\"SOLDIER#1 : Found them? In Mercea? The coconut's tropical!\"\n",
        "pattern1 =r\"(\\w+|#\\d+|\\?|\\!)\"\n",
        "regexp_tokenize(my_str, pattern= pattern1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
        "text = \"Hey , let's explore the topic  NLP.\"\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "ecavLkuNqn5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXyGkL0Z6PeS"
      },
      "source": [
        "# 4. TWEETTOKENIZER\n",
        "\n",
        "Yes, the best way to tokenize tweets is to use the tokenizer built to tokenize tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfjD2cdQdcKR"
      },
      "source": [
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GufDcTaedzFq"
      },
      "source": [
        "tweets =['This is the best #nlp exercise ive found online! #python',\n",
        "         '#NLP is superfun! <3 #learning',\n",
        "         'Thanks @Python :) #nlp #python'] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-Iuf7S1ee9m"
      },
      "source": [
        "pattern1 =r\"#\\w+\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH9GeKu2emo_"
      },
      "source": [
        "regexp_tokenize(tweets[0], pattern1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUH1ghgsewTO"
      },
      "source": [
        "pattern2 =r\"([#|@]\\w+)\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esU5MMC6fZrS"
      },
      "source": [
        "regexp_tokenize(tweets[-1], pattern2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6z7IFJWfiA_"
      },
      "source": [
        "regexp_tokenize(tweets[1], pattern1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6Zbu7L1fqVn"
      },
      "source": [
        "tknzr = TweetTokenizer()\n",
        "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
        "print(all_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHOaZicFgXq1"
      },
      "source": [
        "##NON -ASCII TOKENIZATION##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ddjRN4wgOFF"
      },
      "source": [
        "#Create a string\n",
        "german_text =\" Wann gehen wir zur Pizza? und fahren sie mit vorbei?\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAx2nebzg_ex"
      },
      "source": [
        "all_words = word_tokenize(german_text)\n",
        "print(all_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKB5kHJvhJdb"
      },
      "source": [
        "#tokenize and print only capital words\n",
        "capital_words =r\"[A-Z]\\w+\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR8K8dJxhWk2"
      },
      "source": [
        "print(regexp_tokenize(german_text, capital_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx2f6N5AjBW2"
      },
      "source": [
        "## CHARTING WORD LENGTH WITH NLTK##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WF-E_sRikdD"
      },
      "source": [
        "#PLOTTING A HISTOGRAM WITH MATPLOTLIB\n",
        "from matplotlib import pyplot as plt\n",
        "plt.hist([1,5,5,7,7,7,9])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P18_a90jhVK"
      },
      "source": [
        "#combining NLP data extraction with plotting \n",
        "from nltk.tokenize import word_tokenize\n",
        "words = word_tokenize(\"This is a preety cool tool!\")\n",
        "word_length = [len(w)for w in words]\n",
        "plt.hist(word_length)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgStrq_HkSdf"
      },
      "source": [
        "# WORDS COUNT WITH BAGS OF WORDS\n",
        "\n",
        "# **Basic method for finding topics in a text**\n",
        "\n",
        "* Need to first create tokens using tokenization\n",
        "* ... and then count up all the tokens\n",
        "* The more frequent a word, the more important it might be\n",
        "* Can be a great way to determine the significant words in a text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTiW9wKWkJdy"
      },
      "source": [
        "#BUILDING A COUNTER WITH BAG-OF-WORDS\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "counter = Counter(word_tokenize(\"\"\"The cat is in the box. The cat likes the box. The box is over the cat.\"\"\"))\n",
        "Counter\n",
        "counter.most_common(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FA2J5D9mYpg"
      },
      "source": [
        "tokens = word_tokenize(german_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7poPnM_n_u1"
      },
      "source": [
        "#convert  the tokens into lowercase: lower_tokens\n",
        "lower_tokens = [t.lower() for t in tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fCdANiDopfX"
      },
      "source": [
        "#create a counter with the lowercase tokens bow_simple\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "bow_simple = Counter(lower_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGp1kXjZqTed"
      },
      "source": [
        "#print the 10 most common tokens\n",
        "print(bow_simple.most_common(15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOaVf-vOrHlN"
      },
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "Text preprocessing is a method to clean the text data and make it ready to feed data to the model. Text data contains noise in various forms like emotions, punctuation, text in a different case.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfXpRM2c66rR"
      },
      "source": [
        " #text preprocessing removing stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "Fp97EfOQbRoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUrt1KoZeNIG"
      },
      "source": [
        "text= \"The Cat is in the Box , The Cat likes the Box , The Box is over the Cat.\"\n",
        "tokens = [w for w in word_tokenize(text.lower())if w.isalpha()]\n",
        "no_stops = [t for t in tokens if t not in stopwords.words('english')]\n",
        "Counter(no_stops).most_common(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6v_bFqlctV1"
      },
      "source": [
        "#exersice 2\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNSkdtCOc46E"
      },
      "source": [
        "alpha_only = [t for t in lower_tokens if t.isalpha()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-zzXNO5dAgA"
      },
      "source": [
        "no_stops = [t for t in alpha_only if t not in stopwords.words('english')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMMZIqcndMHm"
      },
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV0e-lBlddNS"
      },
      "source": [
        "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PVcBJxCdv_9"
      },
      "source": [
        "bow = Counter(lemmatized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO6cJTZjfDD4"
      },
      "source": [
        "print(bow.most_common(10))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}